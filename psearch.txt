#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
psearch - fast, configurable CLI search.

Key ideas:
- Text files: search file contents (line-by-line)
- Image files: search filename only (by default)
- -f/--file: search filename only for ALL files
- CSV: render matches in a table (csvlook-ish), with header row and LINE column
- -l/--lines N: include N lines/rows of context above/below each match
- -u/--url: collect URLs from matched output and let you open them by number / all
- If image matches: open a temporary HTML gallery (full screen-ish) in browser

TOML config tip for Windows paths:
- In TOML double-quotes, backslash is an escape.
  Use either:
    "C:\\\\Utils\\\\CSV_XLSX\\\\New folder\\\\datasets"
  or single quotes:
    'C:\Utils\CSV_XLSX\New folder\datasets'
"""

from __future__ import annotations

import argparse
import csv
import os
import re
import sys
import tempfile
import textwrap
import webbrowser
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Optional, Tuple, Dict, Set

from concurrent.futures import ThreadPoolExecutor, as_completed

# Python 3.11+: tomllib in stdlib. (You have 3.13, so you're good.)
import tomllib


# -----------------------------
# ANSI / Color Helpers
# -----------------------------
def _enable_windows_vt_mode() -> None:
    """Enable ANSI escape sequences in Windows terminals (best effort)."""
    if os.name != "nt":
        return
    try:
        import ctypes
        kernel32 = ctypes.windll.kernel32  # type: ignore[attr-defined]
        handle = kernel32.GetStdHandle(-11)  # STD_OUTPUT_HANDLE = -11
        mode = ctypes.c_uint32()
        if kernel32.GetConsoleMode(handle, ctypes.byref(mode)):
            # ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x0004
            kernel32.SetConsoleMode(handle, mode.value | 0x0004)
    except Exception:
        pass


_enable_windows_vt_mode()


def _isatty() -> bool:
    try:
        return sys.stdout.isatty()
    except Exception:
        return False


USE_COLOR = _isatty()


class C:
    RESET = "\x1b[0m" if USE_COLOR else ""
    BOLD = "\x1b[1m" if USE_COLOR else ""
    DIM = "\x1b[2m" if USE_COLOR else ""
    UNDER = "\x1b[4m" if USE_COLOR else ""

    RED = "\x1b[31m" if USE_COLOR else ""
    GREEN = "\x1b[32m" if USE_COLOR else ""
    YELLOW = "\x1b[33m" if USE_COLOR else ""
    BLUE = "\x1b[34m" if USE_COLOR else ""
    MAGENTA = "\x1b[35m" if USE_COLOR else ""
    CYAN = "\x1b[36m" if USE_COLOR else ""


def highlight(text: str, needle_re: re.Pattern) -> str:
    """Highlight all matches of needle_re in text."""
    if not USE_COLOR:
        return text

    def _repl(m: re.Match) -> str:
        return f"{C.BOLD}{C.YELLOW}{m.group(0)}{C.RESET}"

    return needle_re.sub(_repl, text)


# -----------------------------
# Config
# -----------------------------
@dataclass(frozen=True)
class SearchConfig:
    directories: List[Path]
    include_extensions: Optional[Set[str]]  # lowercase, includes dot
    exclude_dir_names: Set[str]             # lowercase
    max_file_size_mb: float
    follow_symlinks: bool
    image_extensions: Set[str]              # lowercase


def load_config(config_path: Path) -> SearchConfig:
    if not config_path.exists():
        raise FileNotFoundError(f"Config not found: {config_path}")

    try:
        data = tomllib.loads(config_path.read_text(encoding="utf-8"))
    except tomllib.TOMLDecodeError as e:
        msg = (
            f"Invalid TOML in config: {config_path}\n"
            f"{e}\n\n"
            f"Windows path tip:\n"
            f"  Use single quotes: 'C:\\Utils\\My Folder'\n"
            f"  OR escape backslashes in double quotes: \"C:\\\\Utils\\\\My Folder\"\n"
        )
        raise ValueError(msg) from e

    s = data.get("search", {})

    dirs = s.get("directories", [])
    if isinstance(dirs, str):
        dirs = [dirs]
    if not isinstance(dirs, list) or not dirs:
        raise ValueError("Config: [search].directories must be a non-empty list")

    directories: List[Path] = []
    for d in dirs:
        if not isinstance(d, str):
            continue
        p = Path(d).expanduser()
        if not p.is_absolute():
            # relative to config location
            p = (config_path.parent / p).resolve()
        else:
            p = p.resolve()
        directories.append(p)

    inc = s.get("include_extensions", None)
    include_exts: Optional[Set[str]] = None
    if isinstance(inc, list) and len(inc) > 0:
        include_exts = {str(x).lower() for x in inc if str(x).startswith(".")}

    exclude = s.get("exclude_dir_names", [])
    if isinstance(exclude, str):
        exclude = [exclude]
    exclude_dir_names = {str(x).lower() for x in exclude}

    max_mb = float(s.get("max_file_size_mb", 10))
    follow = bool(s.get("follow_symlinks", False))

    img = s.get("image_extensions", [".png", ".jpg", ".jpeg", ".gif", ".bmp", ".webp", ".tiff", ".svg"])
    if isinstance(img, str):
        img = [img]
    image_extensions = {str(x).lower() for x in img}

    return SearchConfig(
        directories=directories,
        include_extensions=include_exts,
        exclude_dir_names=exclude_dir_names,
        max_file_size_mb=max_mb,
        follow_symlinks=follow,
        image_extensions=image_extensions,
    )


# -----------------------------
# CLI Help (good-looking)
# -----------------------------
def build_parser() -> argparse.ArgumentParser:
    prog = "psearch"
    desc = "Fast configurable CLI search for everyday development use."

    examples = f"""
{C.BOLD}Examples{C.RESET}
  {C.CYAN}psearch "167"{C.RESET}
      Search configured directories for "167" (text: contents, images: names)

  {C.CYAN}psearch "google" -u{C.RESET}
      Search + collect URLs from matches, then open by number / all

  {C.CYAN}psearch "config" -l 2{C.RESET}
      Include 2 lines (or CSV rows) above & below each match

  {C.CYAN}psearch "report" -f{C.RESET}
      Filename-only search (all filetypes). You can open matched files.

  {C.CYAN}psearch "TODO" -c .\\psearch.toml -i{C.RESET}
      Use custom config + ignore-case search

{C.BOLD}Config file (TOML) location{C.RESET}
  Default: {C.CYAN}.\\psearch.toml{C.RESET}

{C.BOLD}Windows path tip{C.RESET}
  In TOML, backslashes in double quotes are escapes.
  Use:
    directories = ['C:\\Utils\\CSV_XLSX\\New folder\\datasets']
  or:
    directories = ["C:\\\\Utils\\\\CSV_XLSX\\\\New folder\\\\datasets"]
""".strip("\n")

    parser = argparse.ArgumentParser(
        prog=prog,
        description=desc,
        formatter_class=argparse.RawTextHelpFormatter,
        epilog=examples if USE_COLOR else re.sub(r"\x1b\[[0-9;]*m", "", examples),
        add_help=True,
    )

    parser.add_argument("term", help="Search term (regex by default; use --literal for plain text)")
    parser.add_argument(
        "-c", "--config",
        default="psearch.toml",
        help="Path to config TOML (default: ./psearch.toml)",
    )
    parser.add_argument(
        "-i", "--ignore-case",
        action="store_true",
        help="Case-insensitive search",
    )
    parser.add_argument(
        "--literal",
        action="store_true",
        help="Treat term as literal text (not regex)",
    )
    parser.add_argument(
        "-j", "--jobs",
        type=int,
        default=max(4, (os.cpu_count() or 8)),
        help="Worker threads (default: CPU-based)",
    )
    parser.add_argument(
        "-l", "--lines",
        type=int,
        default=0,
        help="Context lines/rows above & below each match (default: 0)",
    )
    parser.add_argument(
        "-u", "--url",
        action="store_true",
        help="Collect URLs from matched lines and let you open them by number / all",
    )
    parser.add_argument(
        "-f", "--file",
        action="store_true",
        help="Filename-only search (skip content search). Lets you open matched files.",
    )
    return parser


# -----------------------------
# Search internals
# -----------------------------
URL_RE = re.compile(r"(https?://[^\s<>\")\]]+)", re.IGNORECASE)


@dataclass
class TextMatch:
    line_no: int
    line: str


@dataclass
class FileResult:
    path: Path
    is_csv: bool
    is_image: bool
    # For text:
    matches: List[TextMatch]          # actual matched lines
    context_blocks: List[List[TextMatch]]  # merged blocks to print
    # For CSV:
    csv_table_blocks: List[str]       # pre-rendered blocks
    # For filename-only:
    filename_hit: bool
    urls_found: Set[str]


def should_skip_dir(dirpath: Path, cfg: SearchConfig) -> bool:
    lower_parts = [p.lower() for p in dirpath.parts]
    for name in cfg.exclude_dir_names:
        if name.lower() in lower_parts:
            return True
    return False


def iter_files(cfg: SearchConfig) -> Iterable[Path]:
    for root in cfg.directories:
        if not root.exists():
            continue
        if root.is_file():
            yield root
            continue

        for dirpath, dirnames, filenames in os.walk(root, followlinks=cfg.follow_symlinks):
            dp = Path(dirpath)
            if should_skip_dir(dp, cfg):
                dirnames[:] = []
                continue

            # prune excluded dir names
            pruned = []
            for dn in dirnames:
                if dn.lower() in cfg.exclude_dir_names:
                    continue
                pruned.append(dn)
            dirnames[:] = pruned

            for fn in filenames:
                p = dp / fn
                if cfg.include_extensions is not None:
                    if p.suffix.lower() not in cfg.include_extensions:
                        continue
                yield p


def compile_needle(term: str, ignore_case: bool, literal: bool) -> re.Pattern:
    flags = re.IGNORECASE if ignore_case else 0
    if literal:
        return re.compile(re.escape(term), flags)
    return re.compile(term, flags)


def is_image_file(path: Path, cfg: SearchConfig) -> bool:
    return path.suffix.lower() in cfg.image_extensions


def is_csv_file(path: Path) -> bool:
    return path.suffix.lower() == ".csv"


def file_too_large(path: Path, cfg: SearchConfig) -> bool:
    try:
        sz = path.stat().st_size
        return sz > (cfg.max_file_size_mb * 1024 * 1024)
    except Exception:
        return True


def merge_line_ranges(line_numbers: List[int], ctx: int, max_line: int) -> List[Tuple[int, int]]:
    """Return merged [start,end] inclusive ranges with context."""
    if not line_numbers:
        return []
    ranges = []
    for ln in sorted(set(line_numbers)):
        a = max(1, ln - ctx)
        b = min(max_line, ln + ctx)
        ranges.append((a, b))
    merged: List[Tuple[int, int]] = []
    cur_a, cur_b = ranges[0]
    for a, b in ranges[1:]:
        if a <= cur_b + 1:
            cur_b = max(cur_b, b)
        else:
            merged.append((cur_a, cur_b))
            cur_a, cur_b = a, b
    merged.append((cur_a, cur_b))
    return merged


def search_filename_only(path: Path, needle: re.Pattern) -> bool:
    return bool(needle.search(path.name))


def read_text_lines(path: Path) -> List[str]:
    # robust-ish decoding
    data = path.read_bytes()
    for enc in ("utf-8", "utf-8-sig", "cp1252", "latin-1"):
        try:
            return data.decode(enc, errors="replace").splitlines()
        except Exception:
            continue
    return data.decode("utf-8", errors="replace").splitlines()


def render_text_blocks(
    path: Path,
    lines: List[str],
    match_lines: Set[int],
    ctx: int,
    needle: re.Pattern,
    collect_urls: bool,
) -> Tuple[List[List[TextMatch]], Set[str]]:
    max_line = len(lines)
    ranges = merge_line_ranges(sorted(match_lines), ctx, max_line)
    blocks: List[List[TextMatch]] = []
    urls: Set[str] = set()

    for a, b in ranges:
        block: List[TextMatch] = []
        for ln in range(a, b + 1):
            raw = lines[ln - 1]
            if collect_urls:
                for u in URL_RE.findall(raw):
                    urls.add(u.rstrip(".,;:)]]\"'"))
            shown = highlight(raw, needle) if (ln in match_lines) else raw
            block.append(TextMatch(ln, shown))
        blocks.append(block)

    return blocks, urls


def format_csv_table_block(
    header: List[str],
    rows_with_line: List[Tuple[int, List[str]]],
    needle: re.Pattern,
    matched_line_numbers: Set[int],
) -> str:
    """
    Create a csvlook-like table as plain text.
    - First column: LINE
    - Highlight matches in matched rows
    """
    # Prepare cell strings (highlight only in matched rows)
    table_rows: List[List[str]] = []
    for ln, row in rows_with_line:
        row_cells = []
        for cell in row:
            s = cell
            if ln in matched_line_numbers:
                s = highlight(s, needle)
            row_cells.append(s)
        table_rows.append([str(ln)] + row_cells)

    cols = ["LINE"] + header
    # compute widths (strip ANSI for width calc)
    ansi_re = re.compile(r"\x1b\[[0-9;]*m")

    def vislen(s: str) -> int:
        return len(ansi_re.sub("", s))

    widths = [vislen(c) for c in cols]
    for r in table_rows:
        for i, c in enumerate(r):
            widths[i] = max(widths[i], vislen(c))

    def fmt_row(items: List[str]) -> str:
        out = []
        for i, it in enumerate(items):
            pad = widths[i] - vislen(it)
            out.append(it + (" " * pad))
        return "  " + " | ".join(out)

    sep = "  " + "-+-".join("-" * w for w in widths)

    lines = [fmt_row(cols), sep]
    for r in table_rows:
        lines.append(fmt_row(r))
    return "\n".join(lines)


def search_csv(
    path: Path,
    needle: re.Pattern,
    ignore_case: bool,
    ctx: int,
    collect_urls: bool,
) -> Tuple[List[str], Set[str]]:
    """
    CSV search:
    - header = first row
    - match if any cell contains term
    - display context rows around each match
    """
    urls: Set[str] = set()

    # Read bytes -> decode robustly
    raw_lines = read_text_lines(path)
    if not raw_lines:
        return [], set()

    # Parse using csv reader from joined text
    text = "\n".join(raw_lines)
    reader = csv.reader(text.splitlines())
    rows = list(reader)
    if not rows:
        return [], set()

    header = rows[0]
    data_rows = rows[1:]

    def row_text(r: List[str]) -> str:
        return ",".join(r)

    matched_line_numbers: Set[int] = set()
    for idx, row in enumerate(data_rows, start=2):  # real file line number
        joined = row_text(row)
        if needle.search(joined):
            matched_line_numbers.add(idx)
        if collect_urls:
            for u in URL_RE.findall(joined):
                urls.add(u.rstrip(".,;:)]]\"'"))

    if not matched_line_numbers:
        return [], urls

    max_line = len(rows)  # header counts as line 1
    ranges = merge_line_ranges(sorted(matched_line_numbers), ctx, max_line)

    blocks: List[str] = []
    for a, b in ranges:
        # rows[a..b] inclusive in file line numbers, but rows list is 0-indexed
        # header is line 1 -> index 0
        # we will build table rows for lines max(2,a)..b (data only); header always shown once per block
        start = max(2, a)
        subset = []
        for ln in range(start, b + 1):
            row_index = ln - 1
            if row_index < 1 or row_index >= len(rows):
                continue
            subset.append((ln, rows[row_index]))
        block = format_csv_table_block(header, subset, needle, matched_line_numbers)
        blocks.append(block)

    return blocks, urls


def search_text_file(
    path: Path,
    needle: re.Pattern,
    ctx: int,
    collect_urls: bool,
    cfg: SearchConfig,
) -> Tuple[List[List[TextMatch]], List[TextMatch], Set[str]]:
    if file_too_large(path, cfg):
        return [], [], set()

    lines = read_text_lines(path)
    match_lines: Set[int] = set()
    matches: List[TextMatch] = []
    urls: Set[str] = set()

    for idx, line in enumerate(lines, start=1):
        if needle.search(line):
            match_lines.add(idx)
            matches.append(TextMatch(idx, highlight(line, needle)))
            if collect_urls:
                for u in URL_RE.findall(line):
                    urls.add(u.rstrip(".,;:)]]\"'"))

    if not matches:
        return [], [], urls

    blocks, ctx_urls = render_text_blocks(path, lines, match_lines, ctx, needle, collect_urls)
    urls |= ctx_urls
    return blocks, matches, urls


def process_one_file(
    path: Path,
    needle: re.Pattern,
    ctx: int,
    collect_urls: bool,
    filename_only: bool,
    cfg: SearchConfig,
) -> Optional[FileResult]:
    try:
        is_img = is_image_file(path, cfg)
        is_csv = is_csv_file(path)

        # Image files -> filename-only search (always)
        if filename_only or is_img:
            hit = search_filename_only(path, needle)
            if not hit:
                return None
            return FileResult(
                path=path,
                is_csv=False,
                is_image=is_img,
                matches=[],
                context_blocks=[],
                csv_table_blocks=[],
                filename_hit=True,
                urls_found=set(),
            )

        # CSV special rendering
        if is_csv:
            blocks, urls = search_csv(path, needle, False, ctx, collect_urls)
            if not blocks:
                return None
            return FileResult(
                path=path,
                is_csv=True,
                is_image=False,
                matches=[],
                context_blocks=[],
                csv_table_blocks=blocks,
                filename_hit=False,
                urls_found=urls,
            )

        # Regular text file
        ctx_blocks, matches, urls = search_text_file(path, needle, ctx, collect_urls, cfg)
        if not matches:
            return None

        return FileResult(
            path=path,
            is_csv=False,
            is_image=False,
            matches=matches,
            context_blocks=ctx_blocks,
            csv_table_blocks=[],
            filename_hit=False,
            urls_found=urls,
        )
    except Exception:
        # fail quietly per-file; keep tool fast
        return None


# -----------------------------
# Output
# -----------------------------
def print_file_result(res: FileResult) -> None:
    print(f"{C.BOLD}{C.CYAN}{res.path}{C.RESET}")

    if res.is_csv:
        for block in res.csv_table_blocks:
            print(block)
            print()
        return

    if res.filename_hit:
        # filename-only mode: just show name
        print(f"  {C.DIM}name:{C.RESET} {res.path.name}")
        print()
        return

    # text mode: print merged context blocks
    for block in res.context_blocks:
        for tm in block:
            # format: "  123 | line..."
            print(f"  {C.DIM}{tm.line_no:>6}{C.RESET} | {tm.line}")
        print()


def prompt_open_urls(urls: List[str]) -> None:
    if not urls:
        return
    print(f"{C.BOLD}{C.MAGENTA}URLs found{C.RESET}")
    for i, u in enumerate(urls, start=1):
        print(f"  {C.DIM}{i:>3}{C.RESET}. {u}")

    print()
    print(f"{C.BOLD}>>Type a number to open (or multiple: 1 3 5). Type 'all' to open all. Enter to skip.{C.RESET}")
    try:
        s = input("> ").strip()
    except (EOFError, KeyboardInterrupt):
        return
    if not s:
        return
    if s.lower() == "all":
        for u in urls:
            webbrowser.open(u)
        return

    parts = s.split()
    for p in parts:
        if not p.isdigit():
            continue
        idx = int(p)
        if 1 <= idx <= len(urls):
            webbrowser.open(urls[idx - 1])


def prompt_open_files(paths: List[Path]) -> None:
    if not paths:
        return
    print(f"{C.BOLD}{C.MAGENTA}Files found{C.RESET}")
    for i, p in enumerate(paths, start=1):
        print(f"  {C.DIM}{i:>3}{C.RESET}. {p}")

    print()
    print(f"{C.BOLD}>>Type a number to open (or multiple: 1 3 5). Type 'all' to open all. Enter to skip.{C.RESET}")
    try:
        s = input("> ").strip()
    except (EOFError, KeyboardInterrupt):
        return
    if not s:
        return

    def open_path(pp: Path) -> None:
        try:
            if os.name == "nt":
                os.startfile(str(pp))  # type: ignore[attr-defined]
            else:
                webbrowser.open(pp.as_uri())
        except Exception:
            pass

    if s.lower() == "all":
        for p in paths:
            open_path(p)
        return

    for p in s.split():
        if not p.isdigit():
            continue
        idx = int(p)
        if 1 <= idx <= len(paths):
            open_path(paths[idx - 1])


def open_image_gallery(image_paths: List[Path]) -> None:
    if not image_paths:
        return

    # build a simple full-screen-ish gallery
    rows = []
    for p in image_paths:
        uri = p.resolve().as_uri()
        name = p.name
        rows.append(f"""
        <div class="row">
          <div class="name">{name}</div>
          <img src="{uri}" alt="{name}">
        </div>
        """)

    html = f"""<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>psearch image matches</title>
  <style>
    body {{
      margin: 0;
      padding: 0;
      font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Arial;
      background: #0b0d10;
      color: #e6edf3;
    }}
    .wrap {{
      width: 100%;
      max-width: 1400px;
      margin: 0 auto;
      padding: 18px;
    }}
    .row {{
      border: 1px solid rgba(255,255,255,0.10);
      border-radius: 12px;
      margin: 14px 0;
      padding: 12px;
      background: rgba(255,255,255,0.03);
    }}
    .name {{
      font-size: 14px;
      opacity: 0.9;
      margin-bottom: 10px;
      word-break: break-all;
    }}
    img {{
      width: 100%;
      height: auto;
      display: block;
      border-radius: 10px;
    }}
  </style>
</head>
<body>
  <div class="wrap">
    <h2 style="margin: 0 0 10px 0;">psearch image matches ({len(image_paths)})</h2>
    {''.join(rows)}
  </div>
</body>
</html>
"""

    tmp = tempfile.NamedTemporaryFile("w", delete=False, suffix=".html", encoding="utf-8")
    try:
        tmp.write(html)
        tmp.close()
        webbrowser.open(Path(tmp.name).as_uri())
    except Exception:
        try:
            tmp.close()
        except Exception:
            pass


# -----------------------------
# Main
# -----------------------------
def main(argv: Optional[List[str]] = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)

    cfg = load_config(Path(args.config).expanduser().resolve())

    try:
        needle = compile_needle(args.term, args.ignore_case, args.literal)
    except re.error as e:
        print(f"{C.RED}Invalid regex:{C.RESET} {e}")
        print(f"Tip: use {C.CYAN}--literal{C.RESET} to search plain text.")
        return 2

    files = list(iter_files(cfg))
    if not files:
        print(f"{C.YELLOW}No files found in configured directories.{C.RESET}")
        return 0

    filename_only = bool(args.file)
    collect_urls = bool(args.url)
    ctx = max(0, int(args.lines))

    results: List[FileResult] = []
    all_urls: Set[str] = set()
    matched_files_for_open: List[Path] = []
    matched_images: List[Path] = []

    # parallel scan
    with ThreadPoolExecutor(max_workers=max(1, int(args.jobs))) as ex:
        futs = [
            ex.submit(process_one_file, p, needle, ctx, collect_urls, filename_only, cfg)
            for p in files
        ]
        for f in as_completed(futs):
            r = f.result()
            if r is None:
                continue
            results.append(r)

    # stable sort by path
    results.sort(key=lambda x: str(x.path).lower())

    if not results:
        print(f"{C.DIM}No matches.{C.RESET}")
        return 0

    # print
    for r in results:
        print_file_result(r)
        all_urls |= r.urls_found
        if filename_only and r.filename_hit:
            matched_files_for_open.append(r.path)
        if r.is_image:
            matched_images.append(r.path)

    # URL open prompt
    if collect_urls:
        urls_sorted = sorted(all_urls)
        prompt_open_urls(urls_sorted)

    # File open prompt (only when -f/--file is used)
    if filename_only:
        prompt_open_files(matched_files_for_open)

    # Image gallery (auto)
    if matched_images:
        open_image_gallery(matched_images)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
